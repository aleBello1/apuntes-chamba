//////////////////////////////////////////TEORIA///////////////////////////////////////////

checar precios: En la pantalla principal de aws buscar el apartado 'precios'.
	- Ver servicios gratuitos: Click en el apartado 'capa gratuita'.

si se dejan prendidos los servicios, se estarán consumiendo los créditos.

AMI: Amazon machine image. Contiene la configuración de software (SO y aplicaciones)
necesaria para lanzar la instancia.

Identity and Access management (IAM): Administración de identidades. Aquí se gestiona los roles
en aws y los permisos que estos tienen.



ZONAS Y REGIONES

El precio de los recursos sí cambia según la región (zona) que elijas. Algunas regiones
suelen tener precios más bajos que otras.

AWS fija distintos precios por región según:
Costos locales de infraestructura y energía
Demanda de clientes en esa región
Costos de operación y logística.

us-east-1 (Virginia) frecuentemente tiene precios más bajos. **************


AMAZON RELATIONAL DATABASE SERVICE (RDS)

Sirve para almacenar nuestras db relaciones como PostgreSQL, MySQL, etc.


ELASTIC COMPUTE CLOUD (EC2)

(Estas explicaciones se escribieron en 2025-2026, supuestamente la interfaz grafica cambio en aprox. 2024, estas explicaciones es para la GUI anterior (de aprox. 2022), entonces puede que las siguientes ya no sean vigentes.
En conclusión, por eso es mejor aprender los comandos y no depender de la GUI)

Es como una vps pero en la nube y de Amazon.

Crear una maquina virtual ---------------------------------------------

En los servicios -> buscar el servicio EC2 -> apartado instances (maquina virtual) -> se debe 
presionar 'lancuh intace' pero antes asegurarse que se tiene seleccionada la zona mas barata
del momento.

Seleccionar un SO para la instancia: Dar un nombre a la instancia (esto es opcional) -> Elegir
	un SO Linux (preferentemente el de Amazon) -> Después elegir específicamente una que 
	una que sea AMI (no elegir alguna que diga server) (También asegurarse que la que se 	
	eliga diga: apto para la capa gratuita).

Seleccionar el tipo de instancia: Seleccionar alguno apto para la capa gratuita y el que 
	tenga mas RAM.

Configurar la red: Cada zona tiene su propia red conocida como: virtual private cloud (vpc) y 
	también tiene subredes que vendrían a ser las subzonas.
	Se debe habilitar la ip publica para que se pueda acceder a la maquina virtual desde
	fuera, por defecto solo se habilita un solo puerto, el puerto 22 el cual permite
	las conexiones remotas mediante ssh. ****
	Le damos click en editar -> en subred va sin preferencias -> crear un grupo de seguridad
	-> add security group role -> tipo: tcp personalizado -> numPuertoApp (aquí ponemos cada
 	puerto que queremos sea consumible desde afuera, pe si tenemos dos microservicios y uno 
	corre en el puerto 5000 y otro en el 8001, abrimos estos dos puertos para que sean 	
	consumibles desde afuera) -> tipo origen: cualquier lugar (obvio desde cualquier lugar
	que tenga la contraseña pem).
	Las reglas de seguridad de entrada sirven cuando un alguien quiere consumir algo del 
	ec2 y las reglas de seguridad de salida sirven cuando el ec2 quiere consumir algo de 
	afuera. ***

Crear par de claves: launch instance -> create a new key -> asignar nombre a las claves -> 
	tipo de par de llaves: rsa -> formato de archivo de clave: .pem -> crear par de claves.
	Se descargará el archivo.
	
Crear instancia:  launch instance -> view intances -> *esperar a que se termine de configurar 
	y levantar la maquina virtual* -> connect 
	-> apartado rdp client -> download remote -> get password -> subir contraseña (que 
	previamente habiamos descargado) -> descrypt password -> copiar password y pegarlo 
	en el software que se descargo.


opcional: crear y configurar vpc ------------------------------

Editar la vpc de una instancia: Entrar al servicio ec2 -> apartado izq: panel de ec2 -> 
	grupos de seguridad -> dar click sobre una vpc para poder configuar las reglas de 
	entrada y salida.

En los servicios: VPC -> Lanzar asistente VPC -> apartado izq: VPC con una sola subred
publica -> seleccionar -> bloque de CIDR ipv6: sin bloque de cidr -> dar un nombre en 
nombre VPC -> zona de disponibilidad: elegir el primero ->  crear VPC.

Asociar instancias al VPC: Irse a las intancias -> lanzar instancias x2 -> dar nombre 
en nombre y etiquetas -> un partado inicio rapido: windows -> en configuraciones de red
dar click en editar -> vpc obligatorio: sleccionar la vpc que creamos -> en sured tambien
-> asignar automaticamente: habilitar -> dar nombre en nombre del guro de seguridad, 
copiarlo y pegarlo como prefijo al nombre de descripcion (dejando la fecha) -> apartado
par de claves: crear nuevo par de claves -> apartaod derecho -> numero de instancias: 2 
-> lanzar instancias
 
Otras configuraciones ---------------------------------------

Ver estado de las instancias: Entrar al servicio ec2 -> apartado izq: panel de ec2 -> instancias
	en ejecución.
	Si se selecciona determinada instancia
	- apartado detalles -> veremos ciertos detalles de la instancia: dirección ipv4 publica,
		dns de ipv4 publica (con estas dos direcciones podemos conectarnos a la 	
		instancia tanto a la terminal asi como lanzar peticiones utsando 
		postman***************), etc.
	- apartado seguridad: Veremos que reglas de entrada y salida tiene esa instancia.

Editar reglas de seguridad de una instancia: Entrar al servicio ec2 -> apartado izq: panel de
 	ec2 -> instancias en ejecución -> seleccionar determinada instancia -> apartado 
	seguridad -> click sobre grupos de seguridad.
	- apartado reglas de entrada -> editar reglas de entrada.
	- apartado reglas de salida -> editar reglas de salida.

Apagar/Detener la maquina virtual: se debe apagar la computadora vitual como normalmente
	se haria (no cerrar la pestaña si no apagarla maquina). 
	Entrar al servicio ec2 -> apartado izq: panel de ec2 -> instancias en ejecución -> 
	seleccionar determinata MV -> actions -> instance state -> 
	- stop instance (para que no consuma creditos). El detenimiento no es inmediato hay 
		que esperar unos minutos. *************
	- finish instance -> Para eliminar la instancia.


Conectarse a la maquina remota (por ssh) ------------------------

°°°°°°°°°° Utilizndo Windows °°°°°°°°°°

Hay varias maneras pero Desde Windows la mejor opción es utilizando putty.
En la pagina de aws poner en el buscador putty para mostrar la documentación de como conectarse
a la maquina remota usando putty.

Entrar al servicio ec2 -> apartado izq: panel de ec2 -> instancias en ejecución -> seleccionar
	determinada instancia -> buscar y presionar el button conectar -> *en un futuro mejorar
	esta parte *

°°°°°°°°°° Utilizando Linux °°°°°°°°°°

Hay varias maneras y una de ellas seria usando una mv con Oracle box que simule una terminal
de Linux.
Otra manera es buscando en Windows a wsl (estará instalado si usamos Docker porque Docker lo
usa forsozamente).


Opcional: Preparación e instalación de Docker -----------------------

Al conectarnos a la MV deberemos actualizar todos los paquetes con: sudo yum update -y

sudo amazon-linux-extras install Docker ó
sudo yum install Docker -> Instala Docker en la MV.

sudo service Docker start -> Levanta el servicio de Docker.

Después instalamos Docker compose, para ello buscar en la pagina oficial de Docker compose
los comandos para instalar Docker compose en una maquina virutal 'Linux standalone binary'.

*Desde la terminal local*
scp -i /ruta/nombreArhivoPem.pem rutaMHost/nombreArchivo.entenxion 
	instance-user-name@instance-public-dns-name rutaMV/ -> Para copiar un archivo desde 
		la maquina local y pegarlo en la maquina remota.
		La idea es copiar y pegar el archivo Docker compose. *
	- instance-user-name: Es el nombre algun usuario dentro de una instancia EC2.
		Por defecto siempre se creara un usuario llamado 'ec2-user'.
	- instance-public-dns-name: Entrar al detalle de una EC2 y en donde diga 'DNS de ipv4
		publica'.

Con esto ya podriamos correr el Docker compose sin problemas. Todos los comandos de Docker se 
deben usar con sudo. *
Pudiese darse el caso de que al usar los comandos se nos arrje el mensaje: 'out of memory' el 
cual implica que estamos al borde de ocupar toda la memoria ram que tiene disponible nuestra
instancia y por tanto ya no obtendremos resultados en nuestros comandos. En ese caso la solución
seria usar un escalamiento horizontal.

*En la maquina vi4rtual*
free -> Nos muestra una tabla donde se ve la memoria ram* total ocupada y disponible.
	Conforme se van levantando los contenedores van ocupando mas y mas meoria, para 
	monitorear la memoria se ocupa este comando.

Para probar con postman que los microservicios, servicios o apps están corriendo usarimos 
como hostname la dns publica y el puerto pertinente: 'dns-publica:puertoApp'.***

Si un Docker compose originalmente estaba conformado por 4 contenedores y por espacio en memoria
se necesita poner 2 contenedores en una mv y los otros 2 contenedores en la otra mv se puede hacer
pero en la instrucción depends_on se quitara los contenedores de los cuales depende.

Se recuerda que, los contenedores que almacenan db puede estar en la instancia pero una mejor
opción seria utilizar un servicio de db en la nube como dinamoDd, rds, mongoDb atlas, etc.


DYNAMO DB

Sirve para almcenar nuestras db no relaciones 


ELASTIC CONTAINER SERVICE (ECS)

(Estas explicaciones se escribieron en 2025-2026, supuestamente la interfaz grafica cambio en aprox. 2024, estas explicaciones es para la GUI anterior (de aprox. 2022), entonces puede que las siguientes ya no sean vigentes.
En conclusión, por eso es mejor aprender los comandos y no depender de la GUI)

Servicio para desplegar contendores.
No tenemos que instalar nada porque ya viene todo lo necesario para trabajar.
Este servicio no se encuentra en la capa gratuita asi que si o si cobrara. ******

Cluster: Es el espacio lógico donde viven y se ejecutan tus aplicaciones en contenedores.

Task Definition: Es la receta donde se describe: que contenedores habrá,  Qué imagenes de Docker 
	usar, cuanta CPU y memoria, las Variables de entorno, Puertos, Roles IAM y Volúmenes
	No corre nada todavía, solo describe. **

Task: es una instancia en ejecución de uno o más contenedores definidos previamente los cuales
	viven dentro de un cluster.
	Para que exista una task, antes debe existir un Task Definition.

Servicio: Controlan como se deben ejecutar las tareas.

Auto-escalado: Es un sistema que responde a esta pregunta, todo el tiempo: “¿Con los recursos 
	que tengo ahora, sigo aguantando bien la carga?”
	Si la respuesta es no → escalo.
	Si la respuesta es sí y me sobra → reduzco.

Crear un cluster: Apartado izq clusters -> créate cluster -> networking only -> asignarle un 
	nombre al cluster -> palomear créate vpc y dejar los valores por defecto -> créate.

Crear un role con permisos de ejecución para ecs: *Ir la pagina de inicio* -> buscar iam -> 
	Apartado izq roles -> En el buscador escribir ecs -> palomear la opción que dice: 
	AWSServiceRoleForECS -> click en crear role -> servicio de aws -> en casos de usaro
	para otros servicios de aws elegir ecs -> palomear elastic container service task ->
	sig -> en el buscador ingresar ecs y elegir AmazonECSTaskExecutionRolePolicy -> sig
	-> crear role -> asignar un nombre al role (este nombre es importante por que se usara
	despues) -> crear role.

Crear un security group (para el elastic file system): Son exactamente la misma configuración
	que para un security group pero en la regla de entrada en lugar de agregar un tipo
	tcp personalizado seria tipo nfs -> source: anywhere ipv4 -> créate security group.

Crear un elastic file system (el que se usa para el volumne): Buscar el servicio llamado elastic 	file system -> crear un sistema de archivos -> asginarle un nombre > seleccionar como 
	vpc el que creamos para el cluster y no el que viene por defecto -> disponibilidad y 
	durabiliad: regional -> personalizar -> sig -> en los grupos de seguridad le colocamos
	en todos lados el grupo de seguridad que creamos (Todos los volúmenes puede ocupar el 
	mismo grupo de seguridad) -> sig -> sig -> créate.

Crear una definición de tarea: Apartado izq. task definitions -> créate new task definition -> 
	fargate (esta opción es serverless, es la mas económica) -> sig. -> asignar nombre a la
	tarea -> elegir el role que creamos en el paso anterior -> operation system family: 
	Linux -> task memory: 1gb -> task cpu: 0.5 vcpu -> add containers -> *Agregar todos los 
	contenedores que estarán en esa tarea* -> *Todos los siguientes valores se pueden 
	extraer del docker compose* -> asignar nombre al contenedor, su imagen (tiene que venir 
	de Docker hub***), variables de ambiente (las mismas que se definen en el Docker compose)
	etc.
		- Un contenedor que almacene una db se debe palomear como essential para indicar
			que otros contenedores dependen de él. 
	Todos los contenedores que estén dentro de la misma tarea se comunican por
	localhost. ***** Entonces si por ejemplo en una tarea hay dos contenedores: uno para la 
	app y otra para su respectiva db, en lugar de conectar a los dos contenedores mediante el 
	nombre del contenedor, se usaría localhost. Este cambio importante, comúnmente se hará
	en las variables de ambiente. 
	Si un contenedor se comunicara con otro contendor que esta en otra tarea, para
	comunicarse idealmente se usara el nombre del dns que se obtiene al asignarle un 
	balanceador de carga a la otra tarea. **
	-> add volumen -> asignarle un nombre al volumen (idealmente se saca este nombre del 
	Docker compose) -> volumen type: efs -> seleccionar el file system id (crear uno antes,
	por cada nuevo volumen se debe usar un único efs) -> add -> regresarse un poco arriba y 	darle click sobre el contenedor que almacena la db -> *en el apartado storage and 	logging* -> mount points: selccionar el elastic file system que creamos -> conitainer 	path: va la ruta donde se almacena lo que queremos guardar en el volumen (se saca del 	Docker compose) -> update -> create.

Crear un servicio: apartado izq task definitions -> actions -> créate service -> fargate ->
	SO: Linux -> en task definition elegir la tarea -> asignar nombre al servicio -> sig -> 
	Asignar un cluster vpc y subred 
		- Las siguientes instrucciones solo se aplican si se uso un load balancer: de
			todos los contenedores que tiene el servicio seleccionar cual contenedor
			nos conectaremos -> add load balancer -> seleccionar el target group.
		- Las siguientes instrucciones solo se aplican si NO se uso un load balancer: 				click en security groups (aquí se definen las reglas de entrada y salida
			para los puertos) -> créate new security group -> type: custom tcp, 
			asignar el puerto donde corre la app -> save- > auto-assign public ip: 
			enabled (para que podamos acceder desde afuera) -> load balancer type:
			none.
	-> next step -> service auto scaling: si el servicio es para una prueba con poner
	do not es suficiente pero si es para producción es configure service with auto scaling
	-> next -> créate service.

Crear un load balancer: Dirigirse al servicio de ec2 (aunque no lo parezca en este servicio se
	crea el load balancer) -> apartado izq. panel de ec2 -> balanceadores de carga ->
	crear un balanceador de carga -> load balancer types: application load balancer:
	créate -> asignarle un nombre -> internet-facing (para acceder desde afuera) -> ipv4 ->
	En cluster vpc asignar el que creamos y no el que viene por defecto -> palomear las 
	subredes -> en security groups seleccionar que hemos creado anteriormente y no el que 
	viene por defecto -> En listeners and routing asignar el target group que creamos (ver 
	la instrucciones de como crear uno si no lo tenemos) -> créate load balancer.

Crear un security group: Dirigirse al servicio de ec2 (aunque no lo parezca en este servicio se
	crea el load balancer) -> apartado izq. panel de ec2 -> security group -> nuevo -> 
	asginarle un nombre al group -> seleccionar el vpc que creamos y no el que viene por 	defecto -> agregar una nueva regla de entrada -> tcp personalizado -> asignar el puerto	
	donde corre la app -> source: anywhere -> crear grupo de seguridad.	 

Crear un target group: Dirigirse al servicio de ec2 (aunque no lo parezca en este servicio se
	crea el load balancer) -> apartado izq. panel de ec2 -> target group -> nuevo ->
	ip addresses -> asignar un nombre al target -> protocol: http; port: puerto de la app ->
	ipv4 -> asignar el vpc que creamos y no el que viene por defecto -> protocol versión: 
	http1 -> next -> créate target group.

Si no se crea un load balancer se generara y se usara una ip_publica y el puerto para consumir la app. * Cada que se levante el servicio, la ip cambiara. Evidentemente esto no sucederá si se tuviera un dominio.  ***
Si se creo y uso un load balancer se genera un dns el cual ese no cambiara no importa cuantas
veces se cree, detenga o reinicie el servicio. Evidentemente también le podríamos poner un dominio.

Determinar la DNS de una servicio: *Aunque no lo parezca, nos debemos de ir al servicio de ec2*
	-> apartado izq. panel de EC2 -> click en balanceadores de carga -> seleccionar 
	determinado balanceador de carga y abajo se mostrara la info de el mismo.

Reiniciar un servicio: aparado izq. task definitions -> seleccionar determinada tarea -> 	seleccionar determinada versión -> click en actions -> update service -> palomear la 
	opción forcé new deployment -> especificar el service name para que coincida con el 
	nombre de la tarea -> next step -> next -> update.


Resolver problema de conexión entre un contenedor app y un contenedor db ----------------

Como los dos conteneores se levantan al mismo teimpo, Se puede presentar la situación de que el contenedor con la app se levante unos instantes antes que el contenedor con la db y el primer
contenedor se intenta conectar y el otro contendor responde y falla todo el proceso. 
Hay varias técnicas para solucionar esta situación.

°°°°°° Usando depends on °°°°°

Esto se ha visto en la teoría de Docker, esta solución no garantiza en un 100% que funcione 
porque NO espera a que MySQL esté listo para aceptar conexiones solo espera a que el contenedor
este levantado.


°°°°°° Usando un delay °°°°°

Es hacer que el contedor con la app tarde en ejecutar/levantar la app. Nuevamente no se garantiza
en un 100% que funcione.

*En el dockerfile*
CMD sleep n && java -jar nombre-microservicioX-0.0.1-SNAPSHOT.jar -> Para levantar la app con
	cierta cantiad de segundos de delay. 


°°°°°° Usando healthcheck + depends_on °°°°°

*Contenedor con la db*
healthcheck:
      test: ["CMD", "mysqladmin", "ping", "-h", "localhost"]
      interval: 10s
      timeout: 5s
      retries: 5

*Contenedor con la app*
depends_on:
      db:
        condition: service_healthy


°°°°°° Usando configuraciones en el perfil °°°°°

spring.datasource.hikari.initialization-fail-timeout=valor -> Controla qué pasa si la BD no está 
	disponible al arrancar la app.
	Posibles valores:
	- -1 -> NO fallar el arranque. La app arranca aunque la BD esté caída.
		Intentará conectarse más tarde, cuando alguien pida una conexión.


Eliminar todos los servicios para que no nos cobre ---------------------

No será necesario eliminar los task definitions porque solo son plantillas que por si solas
no se ejecutan. **

Eliminacion de servicios: ingresar al servicio de ecs -> apartado izq cluster -> click sobre 
	determinado cluster -> seleccionar todos los servicios -> delete.

Eliminar los load balancer: ingresar al servicio de ec2 -> apartado izq. panel de control ->
	load balancer -> seleccionar todos los load balancer -> actions -> eliminar.

Eliminar los elastic file system: ingresar al servicio de efs -> seleccionar todos y elminarlos.

Eliminar los grupos de seguridad: ingresar al servicio de ec2 -> apartado izq. panel de 	control -> grupos de seguridad -> seleccionar todos los grupos de seguridad con excepción
	de los default 

Eliminar los targets group: ingresar al servicio de ec2 -> apartado izq. equilibrio de carga
	-> grupos de destino -> palomearlos -> actions -> delete.

Eliminar cluster: ingresar al servicio de ecs -> apartado izq cluster -> click sobre 
	determinado cluster -> delete cluster.





*Cambiar este nombre*
////////////////////////////////////////////JAVA////////////////////////////////////////////
*por*
/////////////////////////////IMPLEMENTACION DEL DESPLIGUE DE SPRING BOOT////////////////////////





